{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XX1QLU10BNvJ"
   },
   "source": [
    "# K-Graph Partitioning Problem\n",
    "This project is a collection of some algorithm for the Graph Partitioning Problem. The algorithms that we are going to suggest includes:\n",
    "- Backtracking\n",
    "- Integer/Constraint Programming with Google OR-Tools\n",
    "- Spectral Clustering \n",
    "- Greedy Graph Growing Partitioning (GGGP) Algorithm\n",
    "- Local Refinement Technique (Kernighan-Lin)\n",
    "- Multilevel Partitioning using Karlsruhe High Quality Graph Partitioning (KaHIP) module\n",
    "\n",
    "## Problem Statement\n",
    "Given an undirected graph $G = (V,E,w)$. We need to partition $V$ into $k$ subsets $V_1, ..., V_k$ such that:\n",
    "- The $k$ subsets are of nearly equal size. The constraint might be either in the form \n",
    "$$\\max_{i} |V_i| - \\min_{i} |V_i| <= \\alpha $$\n",
    "or \n",
    "$$|V_i| \\le (\\epsilon + 1) \\frac{|V|}{k} \\quad \\forall i $$\n",
    "for some $\\alpha \\in \\mathbf{Z_{++}}$ or $\\epsilon > 0$ are imbalance factors. We denote the two versions of the problems \"$\\alpha$-constrained\" or \"$\\epsilon$-constrained\". \n",
    "- The total weight of edges that connects two different subsets is minimized (the cut size), i.e. $$\\text{minimize} \\sum_{\\{u,v\\}\\in C} w(\\{u,v\\}) \\\\\\text{ for } C = \\{\\{u,v\\}\\in E : u \\in V_i, v\\in V_j, 1 \\le i < j \\le k \\}$$\n",
    "\n",
    "## Input Description\n",
    "For data generation and in backtracking, spectral clustering and local refinement algorithms, we use a weight-adjacency matrix. The weight of edges range from 1 to 100.\n",
    "\n",
    "We also convert our matrix to a compressed sparsed row format to work with IP/CP and the module KaHIP. The data structure consists of four arrays:  \n",
    "- vwgt: the vertex weight array of size n. In our problem, the vertices are of equal weight, hence all elements are unity (1).\n",
    "- xadj, adjncy: adjacency arrays of size (n+1) and (2m) to describe the edges.\n",
    "- adjcwgt: the edge weight array of size 2m. \n",
    "\n",
    "For example, the following arrays:  \n",
    "<code>\n",
    "xadj = [0,2,5,7,9,12]  \n",
    "adjncy = [1,4,0,2,4,1,3,2,4,0,1,3]\n",
    "</code>\n",
    "\n",
    "would describe the following graph:\n",
    "\n",
    "<img src=\"https://drive.google.com/uc?export=view&id=1mcnBJoyxmOyaG5CNFyOyWjV-kyYB_5HL\" \n",
    "     width=\"200\" />\n",
    "\n",
    "The vertices are 0-indexed. The ith vertex is adjacent to vertices in <code>adjncy[xadj[i]:xadj[i+1]]</code>. For example, the vertex $1$ in the graph is adjacent to the edges with index <code> adjncy[2:5] </code>, which are $0,2,4$. Since one edge has to be described from two vertices, the <code>adjncy</code> and <code>adjcwgt</code> arrays are of size (2m)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vCus2wP4tFkK"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "# Input for the problem in the above format\n",
    "input_xadj       = np.array([0,2,5,7,9,12])\n",
    "input_adjncy     = np.array([1,4,0,2,4,1,3,2,4,0,1,3])\n",
    "input_vwgt       = np.array([1,1,1,1,1])\n",
    "input_adjcwgt    = np.array([1,1,1,1,1,1,1,1,1,1,1,1])\n",
    "epsilon          = 0.03 \n",
    "alpha            = 1 \n",
    "k                = 2\n",
    "n = np.shape(input_vwgt)[0] # number of vertices\n",
    "m = np.shape(input_adjcwgt)[0] // 2 # number of edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OHhZ3F16aOVH"
   },
   "outputs": [],
   "source": [
    "# Generate adjacency matrix, since for some of these algorithms to work, n should not be too large\n",
    "\n",
    "adjacency = [[0 for i in range(n)] for j in range(n)]\n",
    "for i in range(n):\n",
    "    adjacency[i][i] = 0\n",
    "for i in range(n):\n",
    "    s,t = input_xadj[i], input_xadj[i+1]\n",
    "    for j in range(s, t):\n",
    "        adjacency[i][input_adjncy[j]] = input_adjcwgt[j]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eImHRkrBuClM"
   },
   "source": [
    "If you want to generate a random graph, activate this cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xqe2xHrVuCP2"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def generate_random_graph(n, m):\n",
    "    graph = [[0 for i in range(n)] for j in range(n)]\n",
    "    edges = set()\n",
    "    for i in range(m):\n",
    "        while True:\n",
    "            u = random.randint(0, n-1)\n",
    "            v = random.randint(0, n-1)\n",
    "            w = random.randint(1, 100)\n",
    "            u, v = min(u,v), max(u,v)\n",
    "            if u != v and (u, v) not in edges:\n",
    "                edges.add((u, v))\n",
    "                graph[u][v] = w\n",
    "                graph[v][u] = w\n",
    "                break\n",
    "    return graph\n",
    "\n",
    "# adjacency = generate_random_graph(n,m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B8tPDP5stWzF"
   },
   "source": [
    "## Backtracking Approach\n",
    "The backtracking algorithm works well for small datasets, and would generate k-partitions of the set {0,...,n-1}. But for larger graphs, time would rise exponentially. The total number of k-partitions would be the Stirling number of the second kind (S(n,k)). \n",
    "\n",
    "We could combine the algorithm with branch and bound method to reduce the number of total partitions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d0aTJ9gafFHU"
   },
   "outputs": [],
   "source": [
    "partition = [0 for i in range(n)]\n",
    "V_max = (1 + epsilon) * math.ceil(n/k)\n",
    "ans_partition = [-1 for i in range(n)]\n",
    "\n",
    "lower_bound = float('inf')\n",
    "\n",
    "def Try(i, mx, ans=0):\n",
    "    global lower_bound, ans_partition\n",
    "    def check_valid_partition(setting=0):\n",
    "        def check_alpha(count):\n",
    "            mn = min(count)\n",
    "            mx = max(count)\n",
    "            if (mx - mn <= alpha):\n",
    "                return True\n",
    "            return False\n",
    "        \n",
    "        def check_epsilon(count):\n",
    "            mx = max(count)\n",
    "            return mx < V_max\n",
    "        \n",
    "        count = [0 for _ in range(k)]\n",
    "        for i in range(n):\n",
    "            count[partition[i]] += 1\n",
    "        if setting == 0:\n",
    "            return check_alpha(count)\n",
    "        else:\n",
    "            return check_epsilon(count)\n",
    "    \n",
    "    if i == n:\n",
    "        if check_valid_partition() == True and ans < lower_bound:\n",
    "            ans_partition = partition[:]\n",
    "            lower_bound = ans\n",
    "    elif i == 0:\n",
    "        partition[0] = 0\n",
    "        Try(1,0,0)\n",
    "    else:\n",
    "        if mx == k-1:\n",
    "            for j in range(k):\n",
    "                new_ans = ans\n",
    "                partition[i] = j\n",
    "                # Update the weight\n",
    "                for t in range(i):\n",
    "                    if partition[t] != j:\n",
    "                        new_ans += adjacency[i][t]\n",
    "                if new_ans <= lower_bound:\n",
    "                    Try(i+1, k-1, new_ans)\n",
    "                else:\n",
    "                    continue\n",
    "\n",
    "        elif (i-mx)+k == n + 1:\n",
    "            new_ans = ans\n",
    "            partition[i] = mx+1\n",
    "            for t in range(i):\n",
    "                if partition[t] != mx+1:\n",
    "                    new_ans += adjacency[i][t]\n",
    "            if new_ans <= lower_bound:\n",
    "                Try(i+1, mx+1, new_ans)\n",
    "            else:\n",
    "                return\n",
    "        else:\n",
    "            for j in range(mx+2):\n",
    "                new_ans = ans\n",
    "                partition[i] = j \n",
    "                for t in range(i):\n",
    "                    if partition[t] != j:\n",
    "                        new_ans += adjacency[i][t]\n",
    "                if new_ans <= lower_bound:\n",
    "                    Try(i+1, max(mx, j), new_ans)\n",
    "                else:\n",
    "                    continue\n",
    "\n",
    "Try(0,0)\n",
    "for i in range(n):\n",
    "    print(\"Vertex {} belongs to partition {}\".format(i, ans_partition[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_ruxp7g1R-Mi"
   },
   "source": [
    "## Integer Programming using OR-Tools\n",
    "OR-Tools is a strong tool for operation research, developed by Google. It is used for constraint programming and integer linear programming (ILP) problems. Today we are going to suggest the mathematical model for both versions of the partitioning problem: the alpha-constrained problem and the epsilon-constrained problem.\n",
    "\n",
    "Let start with installing the OR-Tools package in Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bwdwThvGTHq_"
   },
   "outputs": [],
   "source": [
    "%pip install ortools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fi3huyWtTlPv"
   },
   "source": [
    "Let us begin with introducing an integer programming formulation for the $\\epsilon$-constrained version. First we introduce binary decision variable for all edges and vertices of the graph. For each edge $e=\\{u,v\\}\\in E$, let $e_{uv} \\in \\{0,1\\}$, i.e. whether $e$ is a cut edge. Moreover, for each $v\\in V$ and subset $p$, let $x_{v,p} \\in \\{0.1\\}$ to denote if $v$ is in subset $p$ or not. We have a total of $|E|+k|V|$ variables. \n",
    "\n",
    "The maximum size of a subset should be:\n",
    "$$ V_{max} = (1+\\epsilon) \\lceil \\frac{|V|}{k} \\rceil $$ \n",
    "\n",
    "To ensure a valid partition, we have:\n",
    "$$ \\forall \\{u,v\\} \\in E, \\forall p: e_{uv} \\ge + x_{u,p} - x_{v,p} $$\n",
    "$$ \\forall \\{u,v\\} \\in E, \\forall p: e_{uv} \\ge - x_{u,p} + x_{v,p}  $$\n",
    "$$ \\forall p : \\sum_{v\\in V} x_{v,p} \\le V_{max} \\quad (*)$$ \n",
    "$$ \\forall v\\in V: \\sum_{p} x_{v,p} = 1 $$\n",
    "\n",
    "If we want the partition size to be constrained by alpha instead of epsilon, then $(*)$ should be replaced with\n",
    "\n",
    "$$ \\forall p : \\sum_{v \\in V} x_{v,p} \\le M $$\n",
    "$$ \\forall p : \\sum_{v \\in V} x_{v,p} \\ge m $$\n",
    "$$ M - m \\le \\alpha $$\n",
    "The objective function is:\n",
    "\n",
    "$$ \\text{minimize } \\sum_{\\{u,v\\}\\in E}e_{uv} w(\\{u,v\\}) $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VYq4afoJbKt-"
   },
   "outputs": [],
   "source": [
    "from ortools.linear_solver import pywraplp\n",
    "solver = pywraplp.Solver.CreateSolver('SCIP')\n",
    "solver.set_time_limit(5)\n",
    "infinity = solver.infinity()\n",
    "X = dict()\n",
    "E = dict()\n",
    "for v in range(n):\n",
    "    for p in range(k):\n",
    "        X[(v,p)] = solver.IntVar(0,1,'X[{},{}]'.format(v,p))\n",
    "\n",
    "for u in range(n):\n",
    "    for j in range(input_xadj[u],input_xadj[u+1]):\n",
    "        v = input_adjncy[j] \n",
    "        if u < v:\n",
    "            E[(u,v)] = solver.IntVar(0,1,'E[{},{}]'.format(u,v))\n",
    "\n",
    "print(\"Number of Variables: {}\".format(solver.NumVariables()))\n",
    "\n",
    "for u,v in E:\n",
    "    for p in range(k):\n",
    "        solver.Add(E[u,v] >= X[u,p] - X[v,p])\n",
    "        solver.Add(E[u,v] >= -X[u,p] + X[v,p])\n",
    "\n",
    "mx = solver.IntVar(0,n, 'mx')\n",
    "mn = solver.IntVar(0,n, 'mn')\n",
    "\n",
    "\n",
    "for p in range(k):\n",
    "    constraint = solver.RowConstraint(0,infinity,'')\n",
    "    for v in range(n):\n",
    "        constraint.SetCoefficient(X[v,p], 1)\n",
    "    constraint.SetCoefficient(mn, -1)\n",
    "for p in range(k):\n",
    "    constraint = solver.RowConstraint(0, infinity,'')\n",
    "    for v in range(n):\n",
    "        constraint.SetCoefficient(X[v,p], -1)\n",
    "    constraint.SetCoefficient(mx, 1)\n",
    "    \n",
    "solver.Add(mx - mn <= alpha)\n",
    "\n",
    "for p in range(k):\n",
    "    constraint = solver.RowConstraint(0,infinity,'')\n",
    "    for v in range(n):\n",
    "        constraint.SetCoefficient(X[v,p], 1)\n",
    "        \n",
    "for v in range(n):\n",
    "    constraint = solver.RowConstraint(1,1,'')\n",
    "    for p in range(k):\n",
    "        constraint.SetCoefficient(X[v,p], 1)\n",
    "\n",
    "objective = solver.Objective()\n",
    "for u,v in E:\n",
    "    objective.SetCoefficient(E[(u,v)], int(adjacency[u][v]))\n",
    "objective.SetMinimization()\n",
    "\n",
    "status = solver.Solve()\n",
    "\n",
    "if status == pywraplp.Solver.OPTIMAL or status == pywraplp.Solver.FEASIBLE:\n",
    "    print('Objective Value =', solver.Objective().Value())\n",
    "    for v in range(n):\n",
    "        for p in range(k):\n",
    "            if X[(v,p)].solution_value() == 1.:\n",
    "                print(\"Vertex {} belongs to Partition {}\".format(v,p))\n",
    "    print()\n",
    "    print('Problem solved in %f milliseconds' % solver.wall_time())\n",
    "    print('Problem solved in %d iterations' % solver.iterations())\n",
    "    print('Problem solved in %d branch-and-bound nodes' % solver.nodes())\n",
    "else:\n",
    "    print('The problem does not have an optimal solution.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UDTO7s9x5ccX"
   },
   "source": [
    "# Constraint Programming using Google Ortools\n",
    "\n",
    "We also implemented the above model using a constraint programming solver to benchmark the performance of two solvers on the same modelling. The below implementation is $\\alpha$-constrained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cNgGoyG55kpB"
   },
   "outputs": [],
   "source": [
    "from ortools.sat.python import cp_model\n",
    "\n",
    "model = cp_model.CpModel()\n",
    "\n",
    "X = dict()\n",
    "E = dict()\n",
    "\n",
    "for v in range(n):\n",
    "    for p in range(k):\n",
    "        X[(v,p)] = model.NewIntVar(0,1,'X[{},{}]'.format(v,p))\n",
    "\n",
    "for u in range(n):\n",
    "    for v in range (n): \n",
    "        if adjacency[u][v] !=0 and u < v:\n",
    "            E[(u,v)] = model.NewIntVar(0, 1,'E[{},{}]'.format(u,v))\n",
    "\n",
    "for u,v in E:\n",
    "    for p in range(k):\n",
    "        model.Add(E[u,v] >= X[u,p] - X[v,p])\n",
    "        model.Add(E[u,v] >= -X[u,p] + X[v,p])\n",
    "\n",
    "model.Add(sum(X.values())==n)\n",
    "for v in range (n):\n",
    "    model.Add(sum(X[(v,p)] for p in range (k)) == 1)\n",
    "\n",
    "for p in range (k):\n",
    "    for q in range (k):\n",
    "        model.Add(sum(X[(u,p)] for u in range (n)) - sum(X[(u,q)] for u in range (n)) <= alpha)\n",
    "\n",
    "model.Minimize(sum(E[u,v] * adjacency[u][v] for u, v in E))\n",
    "solver = cp_model.CpSolver()\n",
    "status = solver.Solve(model)\n",
    "if status == cp_model.OPTIMAL or status == cp_model.FEASIBLE:\n",
    "    print('Objective Value =', solver.ObjectiveValue())\n",
    "    for v in range(n):\n",
    "        for p in range(k):\n",
    "            if solver.Value(X[(v,p)]) == 1:\n",
    "                print(\"Vertex {} belongs to Partition {}\".format(v,p))\n",
    "    print()\n",
    "    print('Problem solved in %f milliseconds' % solver.WallTime())\n",
    "\n",
    "else:\n",
    "    print('The problem does not have an optimal solution.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8Xos8C5laOVK"
   },
   "source": [
    "## Spectral Clustering\n",
    "\n",
    "In multivariate statistics, spectral clustering techniques make use of the spectrum (eigenvalues) of the similarity matrix of the data to perform dimensionality reduction before clustering in fewer dimensions. The similarity (affinity) matrix is provided as an input and consists of a quantitative assessment of the relative similarity of each pair of points in the dataset.\n",
    "\n",
    "To implement it, we need to know about some special matrices.\n",
    "\n",
    "First, the weighted adjacency matrix of the graph is the matrix $W = (w_{ij}), i,j=1,...,n$ . $w_{ij}$ equals the weight of the edge that connects 2 vertices i and j. If $w_{ij}$ = 0 this means that the vertices vi and vj are not connected by an edge.\n",
    "\n",
    "Second, the degree matrix $D$ is defined as the diagonal matrix with the degrees $d_1$, . . . , $d_n$ on the diagonal.  The degree of a vertex $v_i$ ∈ $V$ is defined as\n",
    "$d_i = \\sum_{j=1}^n wij $\n",
    "\n",
    "Third, the unnormalized graph Laplacian matrix is defined as $L$ = $D$ - $W$. This matrix has 4 properties:\n",
    "- $L$ is symmetric and positive semi-definite\n",
    "- The smallest eigenvalue of $L$ is 0, the corresponding eigenvector is the constant one vector 1\n",
    "- $L$ has n non-negative, real-valued eigenvalues 0 = $λ_1$ ≤ $λ_2$ ≤ . . . ≤ $λ_n$.\n",
    "- (Number of connected components and the spectrum of $L$) Let $G$ be an undirected graph with non-negative weights. Then the multiplicity $k$ of the eigenvalue 0 of L equals the number of connected components $A_1$, . . . , $A_k$ in the graph. The eigenspace of eigenvalue 0 is spanned by the indicator vectors $1_{A_1}$, . . . , $1_{A_k}$ of those components.\n",
    "\n",
    "Now we would like to state the most common [unnormalized spectral clustering algorithms](https://people.csail.mit.edu/dsontag/courses/ml14/notes/Luxburg07_tutorial_spectral_clustering.pdf).\n",
    "\n",
    "Input: The weighted adjacency matrix $W$ represents the undirected graph $G = (V,E,w)$ \n",
    "- Step 1: Compute the unnormalized Laplacian $L$.\n",
    "- Step 2: Compute the first $k$ eigenvectors $u_1$, . . . , $u_k$ of $L$.\n",
    "- Step 3: Let $U$ ∈ $R^{n×k}$ be the matrix containing the vectors $u_1$, . . . , $u_k$ as columns.\n",
    "- Step 4: For $i = 1, . . . , n$, let $y_i$ ∈ $R^k$ be the vector corresponding to the $i$-th row of $U$.\n",
    "- Step 5: Cluster the points $(y_i)$, $i=1,...,n$ in $R^k$ with the $k$-means algorithm into clusters\n",
    "$C_1$, . . . , $C_k$.\n",
    "\n",
    "Output: Clusters $A_1$, . . . , $A_k$ with $A_i$ = {$j : y_j ∈ C_i$}.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "v5ydKvEX6YHF"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "W = np.array(adjacency)\n",
    "\n",
    "D = [[np.sum(W, axis=0)[i] if i == j else 0 for i in range (n)]\n",
    "     for j in range (n)]\n",
    "L = np.array(D-W)\n",
    "\n",
    "eigenvals, eigenvcts = np.linalg.eig(L)\n",
    "def project_and_transpose(eigenvals, eigenvcts, num_ev):\n",
    "    \"\"\"Select the eigenvectors corresponding to the first \n",
    "    (sorted) num_ev eigenvalues as columns in a data frame.\n",
    "    \"\"\"\n",
    "    eigenvals_sorted_indices = np.argsort(eigenvals)\n",
    "    indices = eigenvals_sorted_indices[: num_ev]\n",
    "\n",
    "    proj_df = pd.DataFrame(eigenvcts[:, indices.squeeze()])\n",
    "    proj_df.columns = ['v_' + str(c) for c in proj_df.columns]\n",
    "    return proj_df\n",
    "\n",
    "proj_df = project_and_transpose(eigenvals, eigenvcts, num_ev=2)\n",
    "\n",
    "def run_k_means(df, n_clusters):\n",
    "    \"\"\"K-means clustering.\"\"\"\n",
    "    k_means = KMeans(random_state=25, n_clusters=n_clusters)\n",
    "    k_means.fit(df)\n",
    "    cluster = k_means.predict(df)\n",
    "    return cluster\n",
    "\n",
    "cluster = run_k_means(proj_df, n_clusters=2)\n",
    "print(cluster)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dONP9gDjquBD"
   },
   "source": [
    "Besides this implementation, there are also several approaches to spectral clustering, some does not even require a full similarity matrix to be constructed to reduce memory. One notable references is from [Ng, Jordan](https://ai.stanford.edu/~ang/papers/nips01-spectral.pdf): \n",
    "\n",
    "In testing, we use the more robust implementation from `scikit-learn` library. Since this is a clustering algorithm, it would work best if the data points form some sort of clusters already, and a huge drawback of this technique is that the cluster sizes might not be the same. However, we think it is still very useful to propose and test this algorithm.\n",
    " \n",
    "Some examples on toy datasets (generator available on [documentation page](https://scikit-learn.org/stable/auto_examples/cluster/plot_cluster_comparison.html#)):\n",
    "\n",
    "<img src=\"https://drive.google.com/uc?export=view&id=1oIqaTD3XuaRDPQs7BSRnIAcsZ4nTyin3\" \n",
    "     width=\"200\" />\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XE8Xa8Zid8Ax"
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import SpectralClustering\n",
    "\n",
    "sc = SpectralClustering(n_clusters=k, affinity='precomputed')\n",
    "partitions = sc.fit_predict(adjacency)\n",
    "\n",
    "print(partitions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "olD9XzULJMB0"
   },
   "source": [
    "## Greedy Graph Growing Partitioning Algorithm\n",
    "\n",
    "For the graph partitioning problem, we propose a very simple greedy framework:\n",
    "\n",
    "    P = (P_0, P_1, ..., P_{k-1})\n",
    "    V' = V\n",
    "    Assign 1 random vertex for each P_i\n",
    "    i = 0\n",
    "    while |V'| > 0:\n",
    "        choose vertex u using a greedy_function(V', P, i, graph)\n",
    "        P_i.add_vertex(u)\n",
    "        V'.remove(u)\n",
    "        p = (p + 1) % k\n",
    "    return P \n",
    "\n",
    "Many greedy algorithms have been proposed and most of them have similar structure. Our greedy algorithm is quite simple: we choose a vertex with minimum total gain, where the gain of a vertex for a partially-formed partition is the total increase in cut-size of that vertex. If many vertices have the same minimun total gain, then tie is broken randomly. \n",
    "\n",
    "    greedy_function(V, P, i, graph):\n",
    "        return argmin_v{total_gain(v, P, i) for v ∈ V}\n",
    "        where total_gain(v, P, i): gain in cutsize if add v into subset i for a partially formed partition P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "TroBS_R1NIkq"
   },
   "outputs": [],
   "source": [
    "def greedy_partition(graph, n, k):\n",
    "    part = [-1] * n\n",
    "    vert = list(range(n))\n",
    "    random.shuffle(vert)\n",
    "\n",
    "    for p in range(k):\n",
    "        u = vert[-1]\n",
    "        part[u] = p\n",
    "        vert.pop()\n",
    "\n",
    "    p = 0\n",
    "\n",
    "    def total_gain(P,p,v,G):\n",
    "        ans = 0\n",
    "        for u in P:\n",
    "            if P[u] != p and P[u] != -1:\n",
    "                ans += G[v][u]\n",
    "                # Gain\n",
    "        return ans\n",
    "\n",
    "    def greedy(V, P, p, G):\n",
    "        m = float('inf')\n",
    "        C = []\n",
    "        gain = [0] * len(V)\n",
    "        for i, v in enumerate(V):\n",
    "            g = total_gain(P,p,v,G)\n",
    "            m = min(m,g)\n",
    "            gain[i] = g\n",
    "        for i in range(len(V)):\n",
    "            if gain[i] == m:\n",
    "                C.append(V[i])\n",
    "        return random.choice(C)\n",
    "\n",
    "    while len(vert) > 0:\n",
    "        b = greedy(vert, part, p, graph)\n",
    "        part[b] = p\n",
    "        vert.remove(b)\n",
    "        p = (p + 1) % k\n",
    "    return part"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eL1b_d0YTmNU"
   },
   "source": [
    "Using this algorithm, the partition is guaranteed to distribute vertices as equally as possible, hence we would not have to worry about alpha nor epsilon. However, its performance is quite poor, hence we would usually run it for several (hundreds) iterations and choose the best result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wJHyCsc8GXr7"
   },
   "source": [
    "## Local Refinement Technique (Kernighan-Lin)\n",
    "\n",
    "The Kernighan-Lin algorithm aims to improve the initial bipartition of a weighted graph. The idea is to iteratively swap vertices between the two partitions such that the total cost is minimized.\n",
    "\n",
    "The algorithm takes an adjacency matrix and an initial bipartition of the vertices. The initial bipartition can be chosen arbitrarily, or in our implementation we improve directly on the partition made by the greedy algorithm above. \n",
    "\n",
    "The algorithm then proceeds in the following steps:\n",
    "\n",
    "- Calculate the degree of each vertex in the graph.\n",
    "\n",
    "- Initialize two empty sets for vertices swapped between partitions.\n",
    "\n",
    "- Create a dictionary to keep track of the gain of each vertex. The gain of a vertex is defined as the reduction in cut weight that would result from swapping the vertex between the two partitions.\n",
    "\n",
    "- Sort the vertices in descending order of their gain.\n",
    "\n",
    "- Swap vertices between the two partitions in order of their gain, until no further improvement is possible. To do this, add the vertex with the highest gain to the set of vertices that will be moved to the other partition. Then recalculate the gains for all remaining vertices and add the vertex with the highest gain to the set of vertices that will be moved to the other partition. Continue this process until no further improvement is possible, i.e. until no more vertices can be swapped to further reduce the cut weight.\n",
    "\n",
    "- Return the final bipartition and the cut weight.\n",
    "\n",
    "The Kernighan-Lin algorithm has a time complexity of $O(n^3)$, where n is the number of vertices in the graph. However, it has been shown to be effective in practice and can produce high-quality partitions for many real-world graphs.\n",
    "\n",
    "\n",
    "<img src=\"https://drive.google.com/uc?export=view&id=1sdGvKHH90C0RX7FdYb-AmVqElo9nMeEe\" \n",
    "     width=\"1000\" />\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JTPS1TGmIjrB"
   },
   "outputs": [],
   "source": [
    "def KL(adj, P0, P1):\n",
    "    # Algorithm takes an adjacency matrix, partition P = (P0, P1)\n",
    "    improvement = True\n",
    "    iteration = 50000\n",
    "\n",
    "    def Ex(u, P):\n",
    "        # external cost\n",
    "        cost = 0\n",
    "        for v in P:\n",
    "            cost += adj[u][v]\n",
    "        return cost\n",
    "\n",
    "    def In(u, P):\n",
    "        # internal cost\n",
    "        cost = 0\n",
    "        for v in P:\n",
    "            cost += adj[u][v]\n",
    "        return cost\n",
    "\n",
    "    def gP_u(u, P, P_):\n",
    "        return Ex(u, P_) - In(u, P)\n",
    "    n = len(adj)\n",
    "    while improvement and iteration > 0:\n",
    "        g = dict()\n",
    "        for u in P0:\n",
    "            g[u] = gP_u(u, P0, P1)\n",
    "        for v in P1:\n",
    "            g[v] = gP_u(v, P1, P0)\n",
    "\n",
    "        L0 = set()\n",
    "        L1 = set()\n",
    "\n",
    "        gains = []\n",
    "\n",
    "        for _ in range(n // 2):\n",
    "            max_gain = -1 * float('inf')\n",
    "            pair = []\n",
    "\n",
    "            for u in P0:\n",
    "                if u not in L0:\n",
    "                    for v in P1:\n",
    "                        if v not in L1:\n",
    "                            gain = g[u] + g[v] - 2 * adj[u][v]\n",
    "                            if gain > max_gain:\n",
    "                                max_gain = gain\n",
    "                                pair = (u,v)\n",
    "            a = pair[0]\n",
    "            b = pair[1]\n",
    "            L0.add(a)\n",
    "            L1.add(b)\n",
    "            gains.append(((a,b), max_gain))\n",
    "\n",
    "            for u in P0:\n",
    "                if u not in L0:\n",
    "                    g[u] += 2 * adj[u][a] - 2 * adj[u][b]\n",
    "\n",
    "            for v in P1:\n",
    "                if v not in L1:\n",
    "                    g[v] += 2 * adj[v][b] - 2 * adj[v][a]\n",
    "\n",
    "        gmax = -1 * float('inf')\n",
    "        jmax = 0\n",
    "        for j in range(1, len(gains) + 1):\n",
    "            gsum = 0\n",
    "            for i in range(j):\n",
    "                gsum += gains[i][1]\n",
    "            if gsum > gmax :\n",
    "                gmax = gsum\n",
    "                jmax = j\n",
    "\n",
    "\n",
    "        if gmax > 0:\n",
    "            for i in range(jmax):\n",
    "                P0.remove(gains[i][0][0])\n",
    "                P0.add(gains[i][0][1])\n",
    "                P1.remove(gains[i][0][1])\n",
    "                P1.add(gains[i][0][0])\n",
    "        else:\n",
    "            improvement = False\n",
    "        iteration -= 1\n",
    "    return P0, P1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "13Qm-b7qrsd_"
   },
   "source": [
    "## Karlsruhe High Quality Graph Partitioning Module\n",
    "\n",
    "This library is a powerful library for multilevel graph partitioning problem. Its multilevel algorithm incorporates many valuable idea, including graph coursening, local search refinements, etc. \n",
    "\n",
    "Multilevel graph partitioning is a very practical and powerful paradigm in solving real-life problem; hence many research were carried out. The general algorithm consists of three phase:\n",
    "\n",
    "* Phase 1: Graph contraction (Coursening)  \n",
    "When an edge {u,v} is collapsed to form a hypervertex, its weight is the sum of weight of vertex u and v. Initially, the weights of all vertices are 1. A rating of an edge illustrate how much it would make sense to contract that edge. A reasonable matching algorithm tries to maximize the sum of the ratings of the contracted edges. This phase stops when the size of the hypergraph is small enough to partition it.  \n",
    "<img src=\"https://drive.google.com/uc?export=view&id=1hJUmyWGIaLm3C9tX6dYKdE4WVxijC5WB\" \n",
    "     width=\"500\" />\n",
    "* Phase 2: Partition the coursened graph  \n",
    "Any partitioning algorithm could be used in this phase, from Integer Programming, Spectral Clustering to Greedy approaches.\n",
    "\n",
    "* Phase 3: Refinement (Uncoursening)  \n",
    "Local search heuristics are used in this phase, when we try to move nodes between blocks to improve the cut size or balance. The 'gain' of a move is the decrease in edge cut if a node is moved to a different block. There are 2 types of refinement algorithm usually used: k-way refinement and 2-way refinement. The former is allowed to move a node to an arbitrary block while the latter is only allowed to move nodes between pair of blocks.  \n",
    "<img src=\"https://drive.google.com/uc?export=view&id=1qf-xy9kotqpp2Q82bhNTHbJevTfgkid2\" \n",
    "     width=\"300\" />\n",
    "\n",
    "To install KaHIP package for Python, please refer to https://github.com/KaHIP/KaHIP where there are detailed instructions. The cell below is to download and build the package with cmake right on Google Colab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8d1mTbdN2CSV"
   },
   "outputs": [],
   "source": [
    "!python3 -m pip install pybind11\n",
    "!git clone https://github.com/KaHIP/KaHIP\n",
    "!KaHIP/compile_withcmake.sh BUILDPYTHONMODULE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oJlgc02AsVvr"
   },
   "source": [
    "Go to directory: <code>KaHIP/deploy</code>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1o2FPsdJrayc"
   },
   "outputs": [],
   "source": [
    "%cd KaHIP/deploy\n",
    "%pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OWSSRm7GsbhX"
   },
   "source": [
    "Then you could import the KaHIP module in your program. There are many configurations of the module that one could use (choose solvers, etc.). To read more about the module and how it works, visit https://kahip.github.io/ and refer to the user guide and publications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5tb1Ivqfkx4f"
   },
   "outputs": [],
   "source": [
    "import kahip\n",
    "\n",
    "#build adjacency array representation of the graph\n",
    "xadj           = input_xadj \n",
    "adjncy         = input_adjncy \n",
    "vwgt           = input_vwgt \n",
    "adjcwgt        = input_adjcwgt \n",
    "supress_output = 0\n",
    "imbalance      = epsilon\n",
    "nblocks        = k\n",
    "seed           = 0\n",
    "\n",
    "# set mode \n",
    "#const int FAST           = 0;\n",
    "#const int ECO            = 1;\n",
    "#const int STRONG         = 2;\n",
    "#const int FASTSOCIAL     = 3;\n",
    "#const int ECOSOCIAL      = 4;\n",
    "#const int STRONGSOCIAL   = 5;\n",
    "mode = 2 \n",
    "\n",
    "edgecut, blocks = kahip.kaffpa(vwgt, xadj, adjcwgt, \n",
    "                              adjncy,  nblocks, imbalance, \n",
    "                              supress_output, seed, mode)\n",
    "\n",
    "print(edgecut)\n",
    "print(blocks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dfQpmRJzgjoB"
   },
   "source": [
    "## Results\n",
    "\n",
    "After testing with random graphs generated with the code above, we benchmarked the algorithms according to time taken and solution found on our randomly generated datasets:\n",
    "    \n",
    "    Small dataset \n",
    "    10vers, 25 edges, 2 clusters, alpha = 2\n",
    "    20vers, 50 edges, 2 clusters, alpha = 2\n",
    "    Test against: backtracking (2 versions); CP (2 versions); IP (2 versions); Greedy (500 & 1000 iterations); Spectral; Greedy+KL\n",
    "\n",
    "    Medium dataset\n",
    "    50vers, 125 edges, 2 clusters, alpha = 2\n",
    "    50vers, 750 edges, 2 clusters, alpha = 2\n",
    "    Test against: CP (2 versions); IP (2 versions); Greedy (500 & 1000 iterations); Spectral; Greedy+KL\n",
    "\n",
    "    Large dataset\n",
    "    100vers, 250 edges, 2 clusters\n",
    "    100vers, 2250 edges, 2 clusters \n",
    "    Test against: CP (eps); IP (eps); Greedy (50 iterations); Spectral; Greedy+KL\n",
    "    100vers, 250 edges, 4 clusters \n",
    "    100vers, 2250 edges, 4 clusters \n",
    "    Test against: CP (eps); IP (eps); Greedy (50 iterations); Spectral\n",
    "    1000vers, 20000 edges, 2 clusters \n",
    "    Test against: Greedy (1 iteration); Spectral; Greedy+KL\n",
    "    1000vers, 20000 edges, 8 clusters \n",
    "    Test against: Greedy (1 iteration); Spectral;\n",
    "\n",
    "After analysing the results, we gain some key insights.\n",
    "\n",
    "First, let's look at some numbers from the small dataset (set2):\n",
    "\n",
    "\n",
    "|      set2           |   avgtime  |   optimal?   |   avgcost  |\n",
    "|---------------------|------------|--------------|------------|\n",
    "|   backtrack(alpha)  |   0.0839   |   yes        |   583      |\n",
    "|   ip(alpha)         |   0.0979   |   yes        |   583      |\n",
    "|   cp(alpha)         |   0.0321   |   yes        |   583      |\n",
    "|   backtrack(eps)    |   0.0487   |   yes        |   523      |\n",
    "|   ip(eps)           |   0.0791   |   yes        |   523      |\n",
    "|   cp(eps)           |   0.0258   |   yes        |   523      |\n",
    "|   greedy(500)       |   0.1089   |   no         |   683      |\n",
    "|   greedy(1000)      |   0.2135   |   no         |   652      |\n",
    "|   greedy+KL         |   0.0044   |   sometimes  |   597      |\n",
    "|   spectral          |   0.0047   |   not balance|   477      |\n",
    "\n",
    "Here, we notice that backtracking, IP and CP all return optimal results in reasonable time. Greedy, on the other hand, does not yield optimal result; however for 500 iterations more we definitely achieve a slightly better result. In spectral clustering, since the cluster is most likely not balanced, it yields an even smaller cost than the optimal result.\n",
    "\n",
    "\n",
    "Let's look at another example from the medium dataset (set3, n=50 and sparse; set4, n=50 and dense):\n",
    "\n",
    "\n",
    "|   set4            |   avgtime       |   avgcost  |\n",
    "|-------------------|-----------------|------------|\n",
    "|   ip(alpha)       |   limit to 10s  |   17549    |\n",
    "|   cp(alpha)       |   limit to 10s  |   15482    |\n",
    "|   ip(eps)         |   limit to 10s  |   17263    |\n",
    "|   cp(eps)         |   limit to 10s  |   15228    |\n",
    "|   greedy(500)     |   1.4687        |   17114    |\n",
    "|   greedy(1000)    |   2.9194        |   16890    |\n",
    "|   greedy+KL       |   0.0062        |   15329    |\n",
    "|   spectral        |   0.0061        |   14431    |\n",
    "\n",
    "In set3, IP and CP can both get to optimal result in less than 1s. On the other hand, greedy alone can yield usable partitions, and with refinement is very good. \n",
    "\n",
    "Last but not least, large test sets (set6.2 and set7.1)\n",
    "\n",
    "|   set6.2      |   avgtime      |   avgcost  |\n",
    "|---------------|----------------|------------|\n",
    "|   ip(eps)     |   limit to 3s  |   85042    |\n",
    "|   cp(eps)     |   limit to 3s  |   78492    |\n",
    "|   greedy(50)  |   1.1566       |   82878    |\n",
    "|   spectral    |   0.0075       |   73201    |\n",
    "\n",
    "|   set7.1      |   avgtime      |   avgcost  |\n",
    "|---------------|----------------|------------|\n",
    "|   greedy(1)   |   21.212       |   504179   |\n",
    "|   greedy+KL   |   64.318       |   377295   |\n",
    "|   spectral    |   0.216        |   390367   |\n",
    "\n",
    "In both set4 and set6.2, IP shows much poorer performance than CP. Moreover, while greedy+KL shows the best in set7.1, it is very slow. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-H7lJmIxG_tc"
   },
   "source": [
    "## Future Work and Remarks\n",
    "\n",
    "For future work, we would like to implement the greedy + Kernighan-Lin algorithm into a recursive bisectioning algorithm to allow it to work for $k = 2^p$. We would also want to look into direct k-way partitioning heuristics and refinement techniques. \n",
    "\n",
    "To conclude, this project is both easy and hard for us, in the sense that the problem itself is easy enough to be able to be understood right away, but quite hard because the amount of research done on the topic is very rich and we hardly could find any room for improvement. However, the results shown above has already given us some very powerful insights about OR-Tools Constraint and Integer Programming solvers, and we also provide valuable techniques and applications that can be applied to different fields, i.e. unsupervised learning, network routing, etc.\n",
    "\n",
    "\n",
    "Source code, graph data and benchmark results can be found here: https://github.com/sanghuynh0929/GraphPartitioning\n",
    "\n",
    "References: https://lume.ufrgs.br/bitstream/handle/10183/67181/000872783.pdf"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
